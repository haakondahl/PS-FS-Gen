This file is a brief statement of the problem, and follow-on information.
---
Sometimes you want a dummy filesystem to test a method on.  In my own use case, I needed to search the entire corporate share edifice for files with certain words in the title, and produce a listing of files to scrub.
Obviously, an undertaking like this is risky, as we were not about to scan the contents of every file on the shares.  A well-tuned initial search could produce a selected list of files to investigate further.  Ideally, that list should be presented in a way that could be handed off to another script or program for in-depth search, or manual review with some assisting automation (even a workflow to open a file, let me deal with it one way or another, then save it into a "done" subdirectory -- options abound).
A well-tuned algorithm needs tuning, and the public shares were not where I wanted to do it.  I wanted to tune my initial search against a local (faster testing), dummy (no chance to affect production work), realistic filesystem.
PowerSHell can create empty files of arbitrarily large size, which will take up space, but which do not require actual writing of data to fill up a file.  The size of these dummy files are respected by everything handy in the (Windows) environment.  Likewise, I leveraged Windows' file association method, relying on the file extension.  By simply creating a dummy file of size 10MB named "dummyfile.pptx", the filesystem and everything else in the environment treated it like an actual 10MB file, and as if it were a valid PowerPoint (.pptx) file -- with the exception of PowerPoint actually opening the file, of course.  
I did some brief statistical poking to see what a coupld of distributions were: How many files are in an average folder?  How many files in a folder are larger than average; how many smaller?  How many folders?  To what depth?  I wound up with a few descriptive statistics that were largely self-similar at most scales.  This is what made my algorithm tuning stand a fighting chance of actually honing in on faster searches.
In order to generate reproducible results, in a transparent manner, I used chains of RND functions to determine sizes and distributions of files and folders in my dummy filesystem.  For example, [pseudocode Array(100) = DO 100:RND(1,RND(1,10))::] yields a Zipf/Benford distribution of whatever the array holds (plus or minus some fence-posting).  I didn't know that.
I know that there are many tools of which I am ignorant -- competent use of REGEX has never been a skill of mine.  No doubt great math exists on depth-first or breadth-first searches in general -- I just wanted to know quick and dirty which rough approaches would work in my particular case.
Nothing prevents a tool like this from being used in combination with better approaches than that I ultimately took, which was a "run powershell looking for dirty words in filenames" scan of the entire filesystem in a major corporate environment.  Obviously if we had better tools, or better training, we would probably have things like this on lock.  But we worked with what we had, and this piece was an important part of not trying stupid things in production.
--
